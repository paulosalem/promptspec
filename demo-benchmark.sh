#!/usr/bin/env bash
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  demo-benchmark.sh â€” Benchmark PromptSpec strategies on GSM8K
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
#  Compares three prompting strategies composed with PromptSpec against
#  the GSM8K math benchmark:
#
#    â€¢ Chain-of-Thought (single-call baseline)
#    â€¢ Self-Consistency (5 samples + majority vote)
#    â€¢ Tree of Thought  (generate â†’ evaluate â†’ synthesize)
#
#  Usage:
#    ./demo-benchmark.sh              # Quick demo (10 samples)
#    ./demo-benchmark.sh --limit 50   # Run with 50 samples
#    ./demo-benchmark.sh --full       # Full GSM8K benchmark (~1300 samples)
#    ./demo-benchmark.sh --model openai/gpt-4o  # Use a different model
#
#  Requirements:
#    â€¢ OPENAI_API_KEY set in environment
#    â€¢ pip install ellements[benchmarking]
#    â€¢ pip install -e .   (promptspec installed in editable mode)
#
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "$0")" && pwd)"
cd "$SCRIPT_DIR"

# â”€â”€ Defaults â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

LIMIT=10
MODEL="openai/gpt-4o-mini"
TASKS="gsm8k"

# â”€â”€ Parse arguments â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

while [[ $# -gt 0 ]]; do
  case "$1" in
    --limit|-l)   LIMIT="$2"; shift 2 ;;
    --full)       LIMIT=""; shift ;;
    --model|-m)   MODEL="$2"; shift 2 ;;
    --tasks|-t)   TASKS="$2"; shift 2 ;;
    --help|-h)
      echo "Usage: $0 [--limit N] [--full] [--model MODEL] [--tasks TASK]"
      echo ""
      echo "Options:"
      echo "  --limit N, -l N   Limit to N samples per task (default: 10)"
      echo "  --full            Run full benchmark (no limit)"
      echo "  --model M, -m M   Model to use (default: openai/gpt-4o-mini)"
      echo "  --tasks T, -t T   Benchmark task (default: gsm8k)"
      echo ""
      echo "Example:"
      echo "  $0 --limit 20 --model openai/gpt-4o"
      exit 0
      ;;
    *) echo "Unknown option: $1"; exit 1 ;;
  esac
done

# â”€â”€ Preflight checks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if [[ -z "${OPENAI_API_KEY:-}" ]]; then
  echo "âŒ OPENAI_API_KEY not set. Export it before running:"
  echo "   export OPENAI_API_KEY=sk-..."
  exit 1
fi

if ! python -c "import lm_eval" 2>/dev/null; then
  echo "âŒ lm-evaluation-harness not installed. Run:"
  echo "   pip install ellements[benchmarking]"
  exit 1
fi

# â”€â”€ Build limit flag â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

LIMIT_FLAG=""
if [[ -n "$LIMIT" ]]; then
  LIMIT_FLAG="--limit $LIMIT"
fi

# â”€â”€ Banner â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

echo ""
echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘  âš¡ PromptSpec Strategy Benchmark Demo                      â•‘"
echo "â•‘                                                              â•‘"
echo "â•‘  Comparing prompting strategies on: $TASKS"
echo "â•‘  Model: $MODEL"
if [[ -n "$LIMIT" ]]; then
echo "â•‘  Samples: $LIMIT (use --full for complete benchmark)"
else
echo "â•‘  Samples: FULL benchmark"
fi
echo "â•‘                                                              â•‘"
echo "â•‘  Strategies:                                                 â•‘"
echo "â•‘    ğŸ“ Chain-of-Thought (single-call baseline)                â•‘"
echo "â•‘    ğŸ² Self-Consistency (5 samples + majority vote)           â•‘"
echo "â•‘    ğŸŒ³ Tree of Thought  (generate â†’ evaluate â†’ synthesize)   â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

# â”€â”€ Run benchmark â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

python scripts/benchmark_strategies.py \
  --specs \
    specs/cot-baseline.promptspec.md \
    specs/self-consistency-solver.promptspec.md \
    specs/tree-of-thought-solver.promptspec.md \
  --tasks "$TASKS" \
  --model "$MODEL" \
  $LIMIT_FLAG \
  --output benchmark-results.json

echo ""
echo "ğŸ“Š Results saved to benchmark-results.json"
echo ""
